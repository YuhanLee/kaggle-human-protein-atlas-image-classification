{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "65KuEOQYX0oI"
   },
   "source": [
    "# Human Protein Atlas Image Classification\n",
    "\n",
    "\n",
    "Image classification of microscope slides based on mixed protein patterns.\n",
    "\n",
    "### Project Description\n",
    "\n",
    "The objective for this project is to determine the locations of protein organelles present in the microscope images. This breaks down into two parts: first, identifying the protein location (in general) from the image, and second, labelling each organelle within the protein.\n",
    "\n",
    "In particular we aim to build a model that can reliably make predictions even when the images contain a mixture of different cell types with different morphologies. \n",
    "\n",
    "### Task \n",
    "\n",
    "The problem is a multi-label image classification task. Each image will contain a mixture of different cell types there are 28 in total. Therefore, the CNN must be able to predict for each image one or many of the 28 labels. \n",
    "\n",
    "The training images that was provided by the kaggle competition includes a train_csv file that contains a list of image ids with the identified protein labels. \n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- keras\n",
    "- tensorflow\n",
    "- numpy\n",
    "- pydot\n",
    "- pandas\n",
    "- OpenCV (opencv-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "RjD80xLQX0oT",
    "outputId": "ce65eeed-4dc2-495b-aedc-87bc428f6b1c"
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Input, Flatten, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import save_img\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "from keras import backend\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image as pil_image # installed by \"pip install Pillow\"\n",
    "\n",
    "import cv2                      # openCV image processing\n",
    "import numpy as np              # linear algebra\n",
    "import pandas as pd             # data processing\n",
    "import matplotlib.pyplot as plt # graphing/visualization\n",
    "import h5py                     # object serialization\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import csv\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8IT9AOJX0os"
   },
   "source": [
    "### Labels\n",
    "All of the 28 labels for identifying the protien. We are predicting protein organelle localization labels for each sample. The dataset was acquired in a standardized way using one imaging modality (confocal microscopy). All image samples are represented by four filters (stored as individual files), the protein of interest (green) plus three cellular landmarks: nucleus (blue), microtubules (red), endoplasmic reticulum (yellow). The green filter should hence be used to predict the label, and the other filters are used as references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xCNdgSFX0ow"
   },
   "outputs": [],
   "source": [
    "NUCLEOPLASM                   = 0\n",
    "NUCLEAR_MEMBRANE              = 1\n",
    "NUCLEOLI                      = 2\n",
    "NUCLEOLI_FIBRILLAR_CENTER     = 3\n",
    "NUCLEAR_SPECKLES              = 4\n",
    "NUCLEAR_BODIES                = 5\n",
    "ENDOPLASMIC_RETICULUM         = 6\n",
    "GOLGI_APPARATUS               = 7\n",
    "PEROXISOMES                   = 8\n",
    "ENDOSOMES                     = 9\n",
    "LYSOSOMES                     = 10\n",
    "INTERMEDIATE_FILAMENTS        = 11\n",
    "ACTIN_FILAMENTS               = 12\n",
    "FOCAL_ADHESION_SITES          = 13\n",
    "MICROTUBULES                  = 14\n",
    "MICROTUBULE_ENDS              = 15\n",
    "CYTOKINETIC_BRIDGE            = 16\n",
    "MITOTIC_SPINDLE               = 17\n",
    "MICROTUBULE_ORGANIZING_CENTER = 18\n",
    "CENTROSOME                    = 19\n",
    "LIPID_DROPLETS                = 20\n",
    "PLASMA_MEMBRANE               = 21\n",
    "CELL_JUNCTIONS                = 22\n",
    "MITOCHONDRIA                  = 23\n",
    "AGGRESOME                     = 24\n",
    "CYTOSOL                       = 25\n",
    "CYTOPLASMIC_BODIES            = 26\n",
    "RODS_AND_RINGS                = 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cy6hSk4LX0o7"
   },
   "source": [
    "### Some Constants\n",
    "Here we define some constants that will define: the local datapath of the training, testing, and label data. We will also define some fundamental inputs to our model, such as: the number of epochs, learning rate, batch size, and the size of the subset we wish to train our data on. As we are constrained by our hardware, we will use a subset of the training data. Finally, we define a random.seed so we maintain the random numbers chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8C04NDFX0pA"
   },
   "outputs": [],
   "source": [
    "LABELS_DIR = 'dataset'\n",
    "TRAIN_DIR = 'dataset/train'\n",
    "TEST_DIR = 'dataset/test'\n",
    "TRAIN_SUBSET_SIZE = 1000 # Selection of images to train from (chosen at random, max size is 31072)\n",
    "LEN_LABELS = 28\n",
    "\n",
    "EPOCHS = 15 # epochs to train for (the more the better)\n",
    "INIT_LR = 1e-3 # initial learning rate for the solver\n",
    "BS = 32 # batch size\n",
    "IMG_SIZE = (512, 512, 1) # 512 by 512 pixels and only one channel (black and white)\n",
    "\n",
    "# random seed\n",
    "random.seed(56732)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AvfKbFHBX0pK"
   },
   "source": [
    "### Image Loading\n",
    "\n",
    "The first step is to take our dataset and prepare it so that it's ready for our model.\n",
    "\n",
    "Images are split into four filters/layers:\n",
    "- **green**: the protein of interest\n",
    "- **blue**: the nucleus\n",
    "- **red**: the microtubules\n",
    "- **yellow**: the endoplasmic reticulum\n",
    "\n",
    "For this project we will mostly be interested in the **green** filter, which will be used to predict the label, while the other filters will be used as references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "cq-iRTUlX0pN",
    "outputId": "cdc15b8e-dcea-44e7-b952-b4120c8a1439"
   },
   "outputs": [],
   "source": [
    "# Get images by layer\n",
    "\n",
    "# training set images\n",
    "train_green  = [(TRAIN_DIR+'/{}').format(i) for i in os.listdir(TRAIN_DIR) if 'green'  in i]\n",
    "\n",
    "# Select TRAIN_SUBSET_SIZE images from the training set at random\n",
    "random.shuffle(train_green)\n",
    "train_green = train_green[:TRAIN_SUBSET_SIZE]\n",
    "train_green_ids = [i[:-10].replace((TRAIN_DIR+'/'),'') \n",
    "                       for i in train_green] # remove '_green.png' and '_green.tif'\n",
    "\n",
    "# Retrieve the other three layers for our subset (NB: unsorted)\n",
    "train_blue =   [(TRAIN_DIR+'/{}').format(i) for i in os.listdir(TRAIN_DIR)\n",
    "                  if ((i[:-9] in train_green_ids) and ('blue' in i))]\n",
    "train_red =    [(TRAIN_DIR+'/{}').format(i) for i in os.listdir(TRAIN_DIR)\n",
    "                  if ((i[:-8] in train_green_ids) and ('red' in i))]\n",
    "train_yellow = [(TRAIN_DIR+'/{}').format(i) for i in os.listdir(TRAIN_DIR)\n",
    "                  if ((i[:-11] in train_green_ids) and ('yellow' in i))]\n",
    "\n",
    "# force garbage collection to make sure memory isn't wasted\n",
    "gc.collect()\n",
    "\n",
    "# test set images\n",
    "test_green  = [(TEST_DIR+'/{}').format(i) for i in os.listdir(TEST_DIR) if 'green'  in i]\n",
    "test_blue   = [(TEST_DIR+'/{}').format(i) for i in os.listdir(TEST_DIR) if 'blue'   in i]\n",
    "test_red    = [(TEST_DIR+'/{}').format(i) for i in os.listdir(TEST_DIR) if 'red'    in i]\n",
    "test_yellow = [(TEST_DIR+'/{}').format(i) for i in os.listdir(TEST_DIR) if 'yellow' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YhGwPb54X0pV"
   },
   "outputs": [],
   "source": [
    "# Testing the above by seeing if the IDs are the same\n",
    "\n",
    "train_green.sort()\n",
    "train_blue.sort()\n",
    "train_red.sort()\n",
    "train_yellow.sort()\n",
    "#print(train_green, end='\\n\\n')\n",
    "#print(train_blue, end='\\n\\n')\n",
    "#print(train_red, end='\\n\\n')\n",
    "#print(train_yellow, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LXf6fgfVjvH0"
   },
   "source": [
    "### Label Loading\n",
    "\n",
    "Loading the labels from *train.csv*. Here we seek to obtain all the labels for the train and test data. These labels could be multi-label, as in indicating more then one protein may be present in any data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtNBfjMfjxV2"
   },
   "outputs": [],
   "source": [
    "train_labels = {} # dictionary with key = photo ID and entry = list of labels\n",
    "\n",
    "# Get all the labels\n",
    "with open('dataset/train.csv') as label_file:\n",
    "    csvreader = csv.reader(label_file, delimiter=',', quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if \"Id\" not in row[0]:\n",
    "            # only take the ones that were taken in the Image Loading part\n",
    "            if any(row[0] in x for x in train_green):\n",
    "                train_labels[row[0]] = row[1:][0].split(' ') # labels are separated by spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8TTKKnnkewLO"
   },
   "source": [
    "### Image Preprocessing\n",
    "We now move onto some image preprocessing after reading the data image .csv files into lists. We will now define a function to produce the input labels and output labels in a unique vector style called one hot encoding. We will also load all the training data into a pandas.dataframe with their labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nhr-zmArr_Ag"
   },
   "source": [
    "#### One Hot Encoding\n",
    "One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.The categorical value represents the numerical value of the entry in the dataset. For example, if a training example is labelled as having NUCLEOLI_FIBRILLAR_CENTER (=3) and ENDOPLASMIC_RETICULUM  (=6), the one_hot_encoded label would be: [0 0 0 1 0 0 1 0....(28 places in total)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kV7Weri7sRWK"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(train_tags):\n",
    "    ''' Creates a one hot encoded dictionary of all labels for each pic'''\n",
    "    encoded = dict()\n",
    "  \n",
    "    for key in train_tags.keys():\n",
    "        # create empty vector\n",
    "        encoding = np.zeros(LEN_LABELS, dtype='uint8')\n",
    "        # mark 1 for each tag in the vector\n",
    "        for tag in train_tags[key]:\n",
    "            encoding[int(tag)] = 1\n",
    "            \n",
    "        encoded[key] = tuple(encoding)\n",
    "  \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load into Pandas\n",
    "We now load every data example, and its corresponding one-hot encoded label into a pandas.dataframe for easier reference later when recalling the label as an answer. We model the training set in this way when providing answers for training the model and testing the validation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Photo ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>00285ce4-bba0-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>003feb6e-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>00626a32-bbab-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>008a780e-bb9e-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0115f496-bbb8-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0167f530-bbc1-11e8-b2bb-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>02199b60-bbc0-11e8-b2bb-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>025b645c-bbbe-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>02c09df8-bbc9-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>030b6ee2-bbbe-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      0   1   2   3   4   5   6   7   8   9   \\\n",
       "Photo ID                                                                       \n",
       "00285ce4-bba0-11e8-b2b9-ac1f6b6435d0   1   0   1   0   0   0   0   0   0   0   \n",
       "003feb6e-bbca-11e8-b2bc-ac1f6b6435d0   1   0   0   0   0   0   0   0   0   0   \n",
       "00626a32-bbab-11e8-b2ba-ac1f6b6435d0   0   0   0   0   0   0   0   0   0   0   \n",
       "008a780e-bb9e-11e8-b2b9-ac1f6b6435d0   0   0   0   0   0   0   0   0   0   0   \n",
       "0115f496-bbb8-11e8-b2ba-ac1f6b6435d0   0   0   1   0   0   0   0   0   0   0   \n",
       "0167f530-bbc1-11e8-b2bb-ac1f6b6435d0   0   0   0   0   0   0   0   0   0   0   \n",
       "02199b60-bbc0-11e8-b2bb-ac1f6b6435d0   0   1   1   0   0   0   0   0   0   0   \n",
       "025b645c-bbbe-11e8-b2ba-ac1f6b6435d0   0   0   0   0   0   0   0   0   0   0   \n",
       "02c09df8-bbc9-11e8-b2bc-ac1f6b6435d0   1   0   0   0   0   0   0   0   0   0   \n",
       "030b6ee2-bbbe-11e8-b2ba-ac1f6b6435d0   0   0   0   0   0   0   0   0   0   0   \n",
       "\n",
       "                                      ...  18  19  20  21  22  23  24  25  26  \\\n",
       "Photo ID                              ...                                       \n",
       "00285ce4-bba0-11e8-b2b9-ac1f6b6435d0  ...   0   0   0   0   0   0   0   0   0   \n",
       "003feb6e-bbca-11e8-b2bc-ac1f6b6435d0  ...   0   0   0   0   0   0   0   0   0   \n",
       "00626a32-bbab-11e8-b2ba-ac1f6b6435d0  ...   0   0   0   0   0   1   0   0   0   \n",
       "008a780e-bb9e-11e8-b2b9-ac1f6b6435d0  ...   0   0   0   1   0   0   0   0   0   \n",
       "0115f496-bbb8-11e8-b2ba-ac1f6b6435d0  ...   0   0   0   0   0   0   0   0   0   \n",
       "0167f530-bbc1-11e8-b2bb-ac1f6b6435d0  ...   0   1   0   0   0   0   0   0   0   \n",
       "02199b60-bbc0-11e8-b2bb-ac1f6b6435d0  ...   0   0   0   0   0   0   0   0   0   \n",
       "025b645c-bbbe-11e8-b2ba-ac1f6b6435d0  ...   0   0   0   0   0   0   0   1   0   \n",
       "02c09df8-bbc9-11e8-b2bc-ac1f6b6435d0  ...   0   0   0   0   0   0   0   1   0   \n",
       "030b6ee2-bbbe-11e8-b2ba-ac1f6b6435d0  ...   0   0   0   1   0   0   0   0   0   \n",
       "\n",
       "                                      27  \n",
       "Photo ID                                  \n",
       "00285ce4-bba0-11e8-b2b9-ac1f6b6435d0   0  \n",
       "003feb6e-bbca-11e8-b2bc-ac1f6b6435d0   0  \n",
       "00626a32-bbab-11e8-b2ba-ac1f6b6435d0   0  \n",
       "008a780e-bb9e-11e8-b2b9-ac1f6b6435d0   0  \n",
       "0115f496-bbb8-11e8-b2ba-ac1f6b6435d0   0  \n",
       "0167f530-bbc1-11e8-b2bb-ac1f6b6435d0   0  \n",
       "02199b60-bbc0-11e8-b2bb-ac1f6b6435d0   0  \n",
       "025b645c-bbbe-11e8-b2ba-ac1f6b6435d0   0  \n",
       "02c09df8-bbc9-11e8-b2bc-ac1f6b6435d0   0  \n",
       "030b6ee2-bbbe-11e8-b2ba-ac1f6b6435d0   0  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_labels = one_hot_encode(train_labels)\n",
    "training_set = pd.DataFrame.from_dict(ohe_labels, orient='index')\n",
    "training_set.index.name = \"Photo ID\"\n",
    "training_set.reset_index()\n",
    "training_set.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Visualization (using matplotlib)\n",
    "\n",
    "As we can see the training set is highly imbalanced.\n",
    "This may be indicative of the rarity of certain protein types.\n",
    "\n",
    "Data imbalance is a well-known problem in Machine Learning. Where some classes in the dataset are more frequent than others, and the neural net just learns to predict the frequent classes.\n",
    "\n",
    "This is a big issue, to mitigate in this case, we can easily balance the data using sampling techniques.\n",
    "By removing some frequent examples(downsampling)\n",
    "By creating more in-frequent examples(upsampling) using image augmentation or any other method.\n",
    "\n",
    "Perhaps a larger data set will show more balanced results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x28a8e0d4908>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Feature Distribution\")\n",
    "plt.xlabel(\"# of occurences\")\n",
    "plt.ylabel(\"protein organelle\")\n",
    "# If this gives a boring graph, increase the subset size to look at more features.\n",
    "training_set.astype(bool).sum(axis=0).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "\n",
    "(see https://keras.io/preprocessing/image/)\n",
    "\n",
    "This will provide real-time data augmentation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation\n",
    "datagen = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "              height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "              horizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and Process (into Keras-ready train and test sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Keras and OpenCV docs\n",
    "\n",
    "def read_and_process(images_paths, ohe_dataframe):\n",
    "    '''\n",
    "    takes a panda dataframe with rows as picture IDs and columns as labels\n",
    "    returns lists for keras (to be used as x and y inputs to the CNN)\n",
    "    '''\n",
    "\n",
    "    images = [] # will be used by keras as x\n",
    "    labels = [] # will be used by keras as y\n",
    "\n",
    "    for img in images_paths:\n",
    "        # build image list using img_to_array() and OpenCV\n",
    "        images.append(img_to_array(cv2.imread(img, cv2.IMREAD_GRAYSCALE))) # load images using cv2.imread()\n",
    "\n",
    "        # build labels/target/tag list\n",
    "        # Check labels at each step and build labels list accordingly\n",
    "        # --> order is important, the lists are meant to be zip()ed\n",
    "        for idx, row in ohe_dataframe.iterrows():\n",
    "            if idx in img:\n",
    "                labels.append(tuple(row))\n",
    "    \n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and Train splitting\n",
    "\n",
    "Split the data from the train set into 80% training and 20% validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 512, 512, 1) (800, 28) (200, 512, 512, 1) (200, 28)\n"
     ]
    }
   ],
   "source": [
    "# WIP\n",
    "# USE sklearn.model_selection import train_test_split FOR THIS PART\n",
    "X, Y = read_and_process(train_green, training_set)\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "print(trainX.shape, trainY.shape, testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x, train_y = read_and_process(train_green, training_set)\n",
    "# test_x,  test_y  = read_and_process(validation_green, training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Setup\n",
    "\n",
    "Now that the preprocessing is out of the way, we can build our CNN structure.\n",
    "\n",
    "This will be a smaller implementation of *VGGNet*, as described here: https://arxiv.org/pdf/1409.1556/\n",
    "\n",
    "#### Input Layer Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "height, width, depth = IMG_SIZE\n",
    "input_shape = (height, width, depth) # derived from image size\n",
    "chanDim = -1\n",
    "classes = LEN_LABELS # number of classification classes\n",
    "\n",
    "# if we are using \"channels first\", update the input shape\n",
    "# and channels dimension\n",
    "if backend.image_data_format() == \"channels_first\":\n",
    "    input_shape = (depth, height, width)\n",
    "    chanDim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv + ReLU + Pool Blocks\n",
    "\n",
    "Increasing filter sizes help increase depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First CONV => RELU => POOL block\n",
    "model.add(Conv2D(16, (3, 3), padding=\"valid\", input_shape=input_shape)) # 32 filters and a 3x3 kernel\n",
    "model.add(Activation(\"relu\")) # standard activation layer\n",
    "model.add(BatchNormalization(axis=chanDim)) # normalize all inputs to the [0, 1] range\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Dropout(0.25)) # dropout will reduce overfitting by randomly dropping 25% of node connections\n",
    "\n",
    "# (CONV => RELU) * 2 => POOL (first double convolution before pooling)\n",
    "model.add(Conv2D(32, (3, 3), padding=\"valid\")) # 64 filters and 3x3 kernel\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(32, (3, 3), padding=\"valid\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # smaller pool size this time to reduce spatial size\n",
    "model.add(Dropout(0.25))\n",
    " \n",
    "# (CONV => RELU) * 2 => POOL (second double convolution before pooling)\n",
    "model.add(Conv2D(64, (3, 3), padding=\"valid\")) # 128 filters and 3x3 kernel\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"valid\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# (CONV => RELU) * 2 => POOL (third double convolution before pooling)\n",
    "model.add(Conv2D(128, (3, 3), padding=\"valid\")) # 128 filters and 3x3 kernel\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"valid\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully Connected + ReLU block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first (and only) set of FC => RELU layers\n",
    "model.add(Flatten()) # Collapses the spatial dimensions of the input into the channel dimensions\n",
    "model.add(Dense(1024)) # dense layer of neurons with 1024 being the dimensionality of the output space\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5)) # Dropout of 50%\n",
    "\n",
    "# Since this is multi-label classification we use the sigmoid activation function\n",
    "model.add(Dense(classes)) # output dimensionality equal to the number of output classes\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "##I think we want \"softmax\" not sigmoid, any picture may be multiple 1's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = SGD(lr=0.01, momentum=0.9) # Stochastic Gradient Descent with 0.01 learning rate\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS) # Adam will provide results faster than SGD with similar quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FBeta Score , Precision, and Recall Functions\n",
    "\n",
    "We must define our performance metrics before compiling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fbeta score for multi-class/label classification\n",
    "def fbeta(y_true, y_pred, beta=2):\n",
    "    # clip predictions\n",
    "    y_pred = backend.clip(y_pred, 0, 1)\n",
    "    # calculate elements\n",
    "    tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
    "    fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "    # calculate precision\n",
    "    p = tp / (tp + fp + backend.epsilon())\n",
    "    # calculate recall\n",
    "    r = tp / (tp + fn + backend.epsilon())\n",
    "    # calculate fbeta, averaged across each class\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
    "    return fbeta_score\n",
    "\n",
    "# calculate precision\n",
    "def precision(y_true, y_pred):\n",
    "    # clip predictions\n",
    "    y_pred = backend.clip(y_pred, 0, 1)\n",
    "    # calculate elements\n",
    "    tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
    "    # calculate precision\n",
    "    p = tp / (tp + fp + backend.epsilon())\n",
    "    \n",
    "    return p\n",
    "\n",
    "# calculate recall\n",
    "def recall(y_true, y_pred):\n",
    "    # clip predictions\n",
    "    y_pred = backend.clip(y_pred, 0, 1)\n",
    "    # calculate elements\n",
    "    tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "    # calculate recall\n",
    "    r = tp / (tp + fn + backend.epsilon())\n",
    "    \n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we're using checkpoints we need to load the model from disk, if possible\n",
    "try:\n",
    "    model = load_model('model.h5')\n",
    "except OSError:\n",
    "    # Otherwise we compile from scratch\n",
    "\n",
    "    # We use binary cross-entropy as this is multilabel multicategory classification\n",
    "    # For metrics we look at accuracy but also a more advanced metric for multilabel classification called the F-beta score\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', precision, recall, fbeta])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 510, 510, 16)      160       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 510, 510, 16)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 510, 510, 16)      64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 170, 170, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 170, 170, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 168, 168, 32)      4640      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 168, 168, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 168, 168, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 166, 166, 32)      9248      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 166, 166, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 166, 166, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 83, 83, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 83, 83, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 81, 81, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 81, 81, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 81, 81, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 79, 79, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 79, 79, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 79, 79, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 39, 39, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 39, 39, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 97344)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              99681280  \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 28)                28700     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 28)                0         \n",
      "=================================================================\n",
      "Total params: 99,784,380\n",
      "Trainable params: 99,781,916\n",
      "Non-trainable params: 2,464\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Checkpoints\n",
    "\n",
    "We'll automatically save the best model to disk so that we can resume training on it later.\n",
    "\n",
    "Different metrics can be chosen to determine the \"best\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_fbeta', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint] # callbacks list used by Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25 samples, validate on 7 samples\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-a7162e52d856>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                 verbose=1)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\olesa026\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\olesa026\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\olesa026\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\olesa026\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\olesa026\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\olesa026\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\olesa026\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\olesa026\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "\n",
    "# With data augmentation\n",
    "#hist = model.fit_generator(datagen.flow(trainX, trainY, batch_size=BS),\n",
    "#            validation_data=(testX, testY),\n",
    "#            steps_per_epoch=(len(trainX) // BS),\n",
    "#            epochs=EPOCHS, verbose=1)\n",
    "\n",
    "# Without data augmentation\n",
    "hist = model.fit(trainX, trainY,\n",
    "                batch_size=BS,\n",
    "                validation_data=(testX, testY),\n",
    "                epochs=EPOCHS, callbacks=callbacks_list,\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagnostics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP\n",
    "def summarize_diagnostics(history):\n",
    "    # plot loss \n",
    "    plt.subplot(411)\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot(history.history['loss'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    # plot accuracy\n",
    "    plt.subplot(412)\n",
    "    plt.title('Fbeta')\n",
    "    plt.plot(history.history['fbeta'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_fbeta'], color='orange', label='test')\n",
    "    #plot precision\n",
    "    plt.subplot(413)\n",
    "    plt.title('Precision')\n",
    "    plt.plot(history.history['precision'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_precision'], color='orange', label='train')\n",
    "    #plot recall\n",
    "    plt.subplot(414)\n",
    "    plt.title('Recall')\n",
    "    plt.plot(history.history['recall'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_recall'], color='orange', label='train')\n",
    "    # save plot to file\n",
    "    filename = sys.argv[0].split('/')[-1]\n",
    "    plt.savefig(filename + '_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_diagnostics(hist)\n",
    "#precision = true positives / (true positives + false positives)\n",
    "#recall = true positives / (true positives + false negatives)\n",
    "#fbeta ==> generalized f1 score derived from both precision and recall averaged over all classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Filter Visualization\n",
    "\n",
    "based on https://keras.io/examples/conv_filter_visualization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"utility function to normalize a tensor.\n",
    "\n",
    "    # Arguments\n",
    "        x: An input tensor.\n",
    "\n",
    "    # Returns\n",
    "        The normalized input tensor.\n",
    "    \"\"\"\n",
    "    return x / (backend.sqrt(backend.mean(backend.square(x))) + backend.epsilon())\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    \"\"\"utility function to convert a float array into a valid uint8 image.\n",
    "\n",
    "    # Arguments\n",
    "        x: A numpy-array representing the generated image.\n",
    "\n",
    "    # Returns\n",
    "        A processed numpy-array, which could be used in e.g. imshow.\n",
    "    \"\"\"\n",
    "    # normalize tensor: center on 0., ensure std is 0.25\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + backend.epsilon())\n",
    "    x *= 0.25\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if backend.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "\n",
    "def process_image(x, former):\n",
    "    \"\"\"utility function to convert a valid uint8 image back into a float array.\n",
    "       Reverses `deprocess_image`.\n",
    "\n",
    "    # Arguments\n",
    "        x: A numpy-array, which could be used in e.g. imshow.\n",
    "        former: The former numpy-array.\n",
    "                Need to determine the former mean and variance.\n",
    "\n",
    "    # Returns\n",
    "        A processed numpy-array representing the generated image.\n",
    "    \"\"\"\n",
    "    if backend.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((2, 0, 1))\n",
    "    return (x / 255 - 0.5) * 4 * former.std() + former.mean()\n",
    "\n",
    "\n",
    "def visualize_layer(model,\n",
    "                    layer_name,\n",
    "                    step=1.,\n",
    "                    epochs=15,\n",
    "                    upscaling_steps=9,\n",
    "                    upscaling_factor=1.2,\n",
    "                    output_dim=(412, 412),\n",
    "                    filter_range=(0, None)):\n",
    "    \"\"\"Visualizes the most relevant filters of one conv-layer in a certain model.\n",
    "\n",
    "    # Arguments\n",
    "        model: The model containing layer_name.\n",
    "        layer_name: The name of the layer to be visualized.\n",
    "                    Has to be a part of model.\n",
    "        step: step size for gradient ascent.\n",
    "        epochs: Number of iterations for gradient ascent.\n",
    "        upscaling_steps: Number of upscaling steps.\n",
    "                         Starting image is in this case (80, 80).\n",
    "        upscaling_factor: Factor to which to slowly upgrade\n",
    "                          the image towards output_dim.\n",
    "        output_dim: [img_width, img_height] The output image dimensions.\n",
    "        filter_range: Tupel[lower, upper]\n",
    "                      Determines the to be computed filter numbers.\n",
    "                      If the second value is `None`,\n",
    "                      the last filter will be inferred as the upper boundary.\n",
    "    \"\"\"\n",
    "\n",
    "    def _generate_filter_image(input_img,\n",
    "                               layer_output,\n",
    "                               filter_index):\n",
    "        \"\"\"Generates image for one particular filter.\n",
    "\n",
    "        # Arguments\n",
    "            input_img: The input-image Tensor.\n",
    "            layer_output: The output-image Tensor.\n",
    "            filter_index: The to be processed filter number.\n",
    "                          Assumed to be valid.\n",
    "\n",
    "        #Returns\n",
    "            Either None if no image could be generated.\n",
    "            or a tuple of the image (array) itself and the last loss.\n",
    "        \"\"\"\n",
    "        s_time = time.time()\n",
    "\n",
    "        # we build a loss function that maximizes the activation\n",
    "        # of the nth filter of the layer considered\n",
    "        if backend.image_data_format() == 'channels_first':\n",
    "            loss = backend.mean(layer_output[:, filter_index, :, :])\n",
    "        else:\n",
    "            loss = backend.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "        # we compute the gradient of the input picture wrt this loss\n",
    "        grads = backend.gradients(loss, input_img)[0]\n",
    "\n",
    "        # normalization trick: we normalize the gradient\n",
    "        grads = normalize(grads)\n",
    "\n",
    "        # this function returns the loss and grads given the input picture\n",
    "        iterate = backend.function([input_img], [loss, grads])\n",
    "\n",
    "        # we start from a gray image with some random noise\n",
    "        intermediate_dim = tuple(\n",
    "            int(x / (upscaling_factor ** upscaling_steps)) for x in output_dim)\n",
    "        if backend.image_data_format() == 'channels_first':\n",
    "            input_img_data = np.random.random(\n",
    "                (1, 3, intermediate_dim[0], intermediate_dim[1]))\n",
    "        else:\n",
    "            input_img_data = np.random.random(\n",
    "                (1, intermediate_dim[0], intermediate_dim[1], 3))\n",
    "        input_img_data = (input_img_data - 0.5) * 20 + 128\n",
    "\n",
    "        # Slowly upscaling towards the original size prevents\n",
    "        # a dominating high-frequency of the to visualized structure\n",
    "        # as it would occur if we directly compute the 412d-image.\n",
    "        # Behaves as a better starting point for each following dimension\n",
    "        # and therefore avoids poor local minima\n",
    "        for up in reversed(range(upscaling_steps)):\n",
    "            # we run gradient ascent for e.g. 20 steps\n",
    "            for _ in range(epochs):\n",
    "                loss_value, grads_value = iterate([input_img_data])\n",
    "                input_img_data += grads_value * step\n",
    "\n",
    "                # some filters get stuck to 0, we can skip them\n",
    "                if loss_value <= backend.epsilon():\n",
    "                    return None\n",
    "\n",
    "            # Calculate upscaled dimension\n",
    "            intermediate_dim = tuple(\n",
    "                int(x / (upscaling_factor ** up)) for x in output_dim)\n",
    "            # Upscale\n",
    "            img = deprocess_image(input_img_data[0])\n",
    "            img = np.array(pil_image.fromarray(img).resize(intermediate_dim,\n",
    "                                                           pil_image.BICUBIC))\n",
    "            input_img_data = np.expand_dims(\n",
    "                process_image(img, input_img_data[0]), 0)\n",
    "\n",
    "        # decode the resulting input image\n",
    "        img = deprocess_image(input_img_data[0])\n",
    "        e_time = time.time()\n",
    "        print('Costs of filter {:3}: {:5.0f} ( {:4.2f}s )'.format(filter_index,\n",
    "                                                                  loss_value,\n",
    "                                                                  e_time - s_time))\n",
    "        return img, loss_value\n",
    "\n",
    "    def _draw_filters(filters, n=None):\n",
    "        \"\"\"Draw the best filters in a nxn grid.\n",
    "\n",
    "        # Arguments\n",
    "            filters: A List of generated images and their corresponding losses\n",
    "                     for each processed filter.\n",
    "            n: dimension of the grid.\n",
    "               If none, the largest possible square will be used\n",
    "        \"\"\"\n",
    "        if n is None:\n",
    "            n = int(np.floor(np.sqrt(len(filters))))\n",
    "\n",
    "        # the filters that have the highest loss are assumed to be better-looking.\n",
    "        # we will only keep the top n*n filters.\n",
    "        filters.sort(key=lambda x: x[1], reverse=True)\n",
    "        filters = filters[:n * n]\n",
    "\n",
    "        # build a black picture with enough space for\n",
    "        # e.g. our 8 x 8 filters of size 412 x 412, with a 5px margin in between\n",
    "        MARGIN = 5\n",
    "        width = n * output_dim[0] + (n - 1) * MARGIN\n",
    "        height = n * output_dim[1] + (n - 1) * MARGIN\n",
    "        stitched_filters = np.zeros((width, height, 3), dtype='uint8')\n",
    "\n",
    "        # fill the picture with our saved filters\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                img, _ = filters[i * n + j]\n",
    "                width_margin = (output_dim[0] + MARGIN) * i\n",
    "                height_margin = (output_dim[1] + MARGIN) * j\n",
    "                stitched_filters[\n",
    "                    width_margin: width_margin + output_dim[0],\n",
    "                    height_margin: height_margin + output_dim[1], :] = img\n",
    "\n",
    "        # save the result to disk\n",
    "        save_img('vgg_{0:}_{1:}x{1:}.png'.format(layer_name, n), stitched_filters)\n",
    "\n",
    "    # this is the placeholder for the input images\n",
    "    assert len(model.inputs) == 1\n",
    "    input_img = model.inputs[0]\n",
    "\n",
    "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "\n",
    "    output_layer = layer_dict[layer_name]\n",
    "    assert isinstance(output_layer, layers.Conv2D)\n",
    "\n",
    "    # Compute to be processed filter range\n",
    "    filter_lower = filter_range[0]\n",
    "    filter_upper = (filter_range[1]\n",
    "                    if filter_range[1] is not None\n",
    "                    else len(output_layer.get_weights()[1]))\n",
    "    assert(filter_lower >= 0\n",
    "           and filter_upper <= len(output_layer.get_weights()[1])\n",
    "           and filter_upper > filter_lower)\n",
    "    print('Compute filters {:} to {:}'.format(filter_lower, filter_upper))\n",
    "\n",
    "    # iterate through each filter and generate its corresponding image\n",
    "    processed_filters = []\n",
    "    for f in range(filter_lower, filter_upper):\n",
    "        img_loss = _generate_filter_image(input_img, output_layer.output, f)\n",
    "\n",
    "        if img_loss is not None:\n",
    "            processed_filters.append(img_loss)\n",
    "\n",
    "    print('{} filter processed.'.format(len(processed_filters)))\n",
    "    # Finally draw and store the best filters to disk\n",
    "    _draw_filters(processed_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Script\n",
    "\n",
    "This script will help us visualize what the convolutional layers are learning.\n",
    "\n",
    "Unfortunately because of a TensorFlow issue it only works if images are read\n",
    "with cv2.IMREAD_COLOR and an RGB channel depth in image_size. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv2d_1', 'conv2d_2', 'conv2d_3', 'conv2d_4', 'conv2d_5']\n"
     ]
    }
   ],
   "source": [
    "conv_layers = [l.name for l in model.layers if 'conv2d' in l.name]\n",
    "print(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute filters 0 to 64\n",
      "Costs of filter   2:     9 ( 8.83s )\n",
      "Costs of filter   4:     7 ( 9.06s )\n",
      "Costs of filter   5:    13 ( 8.96s )\n",
      "Costs of filter   9:    10 ( 9.17s )\n",
      "Costs of filter  14:     4 ( 9.64s )\n",
      "Costs of filter  19:    14 ( 9.95s )\n",
      "Costs of filter  26:     6 ( 10.89s )\n",
      "Costs of filter  33:     7 ( 22.39s )\n",
      "Costs of filter  34:     7 ( 16.06s )\n",
      "Costs of filter  36:    11 ( 13.84s )\n",
      "Costs of filter  37:    10 ( 14.36s )\n",
      "Costs of filter  39:    16 ( 15.93s )\n",
      "Costs of filter  45:    13 ( 21.25s )\n",
      "Costs of filter  52:     8 ( 15.19s )\n",
      "Costs of filter  56:    13 ( 16.15s )\n",
      "Costs of filter  57:     9 ( 17.57s )\n",
      "Costs of filter  59:    11 ( 17.04s )\n",
      "Costs of filter  60:    13 ( 21.69s )\n",
      "Costs of filter  61:     7 ( 21.89s )\n"
     ]
    }
   ],
   "source": [
    "for layer_name in conv_layers[2:]:\n",
    "    try:\n",
    "        visualize_layer(model, layer_name)\n",
    "    except Exception as e:\n",
    "        print(\"This model isn't compatible with the script. Sorry!\")\n",
    "        print(str(e))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "protein_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
