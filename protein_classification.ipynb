{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "65KuEOQYX0oI"
   },
   "source": [
    "# Human Protein Atlas Image Classification\n",
    "\n",
    "\n",
    "Image classification of microscope slides based on mixed protein patterns.\n",
    "\n",
    "### Project Description\n",
    "\n",
    "The objective for this project is to determine the locations of protein organelles present in the microscope images. This breaks down into two parts: first, identifying the protein location (in general) from the image, and second, labelling each organelle within the protein.\n",
    "\n",
    "In particular we aim to build a model that can reliably make predictions even when the images contain a mixture of different cell types with different morphologies. \n",
    "\n",
    "### Task \n",
    "\n",
    "The problem is a multi-label image classification task. Each image will contain a mixture of different cell types there are 28 in total. Therefore, the CNN must be able to predict for each image one or many of the 28 labels. \n",
    "\n",
    "The training images that was provided by the kaggle competition includes a train_csv file that contains a list of image ids with the identified protein labels. \n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- keras\n",
    "- tensorflow\n",
    "- numpy\n",
    "- pydot\n",
    "- pandas\n",
    "- OpenCV (opencv-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "RjD80xLQX0oT",
    "outputId": "ce65eeed-4dc2-495b-aedc-87bc428f6b1c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Input, Flatten, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2                      # openCV image processing\n",
    "import pickle                   # object serialization\n",
    "import numpy as np              # linear algebra\n",
    "import pandas as pd             # data processing\n",
    "import pydot                    # graphing/visualization\n",
    "import matplotlib.pyplot as plt # graphing/visualization\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8IT9AOJX0os"
   },
   "source": [
    "### Labels\n",
    "All of the 28 labels for identifying the protien. We are predicting protein organelle localization labels for each sample. The dataset was acquired in a standardized way using one imaging modality (confocal microscopy). All image samples are represented by four filters (stored as individual files), the protein of interest (green) plus three cellular landmarks: nucleus (blue), microtubules (red), endoplasmic reticulum (yellow). The green filter should hence be used to predict the label, and the other filters are used as references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xCNdgSFX0ow"
   },
   "outputs": [],
   "source": [
    "NUCLEOPLASM                   = 0\n",
    "NUCLEAR_MEMBRANE              = 1\n",
    "NUCLEOLI                      = 2\n",
    "NUCLEOLI_FIBRILLAR_CENTER     = 3\n",
    "NUCLEAR_SPECKLES              = 4\n",
    "NUCLEAR_BODIES                = 5\n",
    "ENDOPLASMIC_RETICULUM         = 6\n",
    "GOLGI_APPARATUS               = 7\n",
    "PEROXISOMES                   = 8\n",
    "ENDOSOMES                     = 9\n",
    "LYSOSOMES                     = 10\n",
    "INTERMEDIATE_FILAMENTS        = 11\n",
    "ACTIN_FILAMENTS               = 12\n",
    "FOCAL_ADHESION_SITES          = 13\n",
    "MICROTUBULES                  = 14\n",
    "MICROTUBULE_ENDS              = 15\n",
    "CYTOKINETIC_BRIDGE            = 16\n",
    "MITOTIC_SPINDLE               = 17\n",
    "MICROTUBULE_ORGANIZING_CENTER = 18\n",
    "CENTROSOME                    = 19\n",
    "LIPID_DROPLETS                = 20\n",
    "PLASMA_MEMBRANE               = 21\n",
    "CELL_JUNCTIONS                = 22\n",
    "MITOCHONDRIA                  = 23\n",
    "AGGRESOME                     = 24\n",
    "CYTOSOL                       = 25\n",
    "CYTOPLASMIC_BODIES            = 26\n",
    "RODS_AND_RINGS                = 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cy6hSk4LX0o7"
   },
   "source": [
    "### Some Constants\n",
    "Here we define some constants that will define: the local datapath of the training, testing, and label data. We will also define some fundamental inputs to our model, such as: the number of epochs, learning rate, batch size, and the size of the subset we wish to train our data on. As we are constrained by our hardware, we will use a subset of the training data. Finally, we define a random.seed so we maintain the random numbers chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8C04NDFX0pA"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b23d5ba09b53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# random seed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m56732\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "LABELS_DIR = 'dataset'\n",
    "TRAIN_DIR = 'dataset/train'\n",
    "TEST_DIR = 'dataset/test'\n",
    "TRAIN_SUBSET_SIZE = 100 # Selection of images to train from (chosen at random, max size is 31072)\n",
    "LEN_LABELS = 28\n",
    "\n",
    "EPOCHS = 15 # epochs to train for (the more the better)\n",
    "INIT_LR = 1e-3 # initial learning rate for the solver\n",
    "BS = 16 # batch size\n",
    "IMG_SIZE = (512, 512, 1) # 512 by 512 pixels and only one channel (black and white)\n",
    "\n",
    "# random seed\n",
    "random.seed(56732)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AvfKbFHBX0pK"
   },
   "source": [
    "### Image Loading\n",
    "\n",
    "The first step is to take our dataset and prepare it so that it's ready for our model.\n",
    "\n",
    "Images are split into four filters/layers:\n",
    "- **green**: the protein of interest\n",
    "- **blue**: the nucleus\n",
    "- **red**: the microtubules\n",
    "- **yellow**: the endoplasmic reticulum\n",
    "\n",
    "For this project we will mostly be interested in the **green** filter, which will be used to predict the label, while the other filters will be used as references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "cq-iRTUlX0pN",
    "outputId": "cdc15b8e-dcea-44e7-b952-b4120c8a1439"
   },
   "outputs": [],
   "source": [
    "# Get images by layer\n",
    "\n",
    "# training set images\n",
    "train_green  = [(TRAIN_DIR+'/{}').format(i) for i in os.listdir(TRAIN_DIR) if 'green'  in i]\n",
    "\n",
    "# Select TRAIN_SUBSET_SIZE images from the training set at random\n",
    "random.shuffle(train_green)\n",
    "train_green = train_green[:TRAIN_SUBSET_SIZE]\n",
    "train_green_ids = [i[:-10].replace((TRAIN_DIR+'/'),'') \n",
    "                       for i in train_green] # remove '_green.png' and '_green.tif'\n",
    "\n",
    "# Retrieve the other three layers for our subset (NB: unsorted)\n",
    "train_blue =   [(TRAIN_DIR+'/{}').format(i) for i in os.listdir(TRAIN_DIR)\n",
    "                  if ((i[:-9] in train_green_ids) and ('blue' in i))]\n",
    "train_red =    [(TRAIN_DIR+'/{}').format(i) for i in os.listdir(TRAIN_DIR)\n",
    "                  if ((i[:-8] in train_green_ids) and ('red' in i))]\n",
    "train_yellow = [(TRAIN_DIR+'/{}').format(i) for i in os.listdir(TRAIN_DIR)\n",
    "                  if ((i[:-11] in train_green_ids) and ('yellow' in i))]\n",
    "\n",
    "# force garbage collection to make sure memory isn't wasted\n",
    "gc.collect()\n",
    "\n",
    "# test set images\n",
    "test_green  = [(TEST_DIR+'/{}').format(i) for i in os.listdir(TEST_DIR) if 'green'  in i]\n",
    "test_blue   = [(TEST_DIR+'/{}').format(i) for i in os.listdir(TEST_DIR) if 'blue'   in i]\n",
    "test_red    = [(TEST_DIR+'/{}').format(i) for i in os.listdir(TEST_DIR) if 'red'    in i]\n",
    "test_yellow = [(TEST_DIR+'/{}').format(i) for i in os.listdir(TEST_DIR) if 'yellow' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YhGwPb54X0pV"
   },
   "outputs": [],
   "source": [
    "# Testing the above by seeing if the IDs are the same\n",
    "\n",
    "train_green.sort()\n",
    "train_blue.sort()\n",
    "train_red.sort()\n",
    "train_yellow.sort()\n",
    "#print(train_green, end='\\n\\n')\n",
    "#print(train_blue, end='\\n\\n')\n",
    "#print(train_red, end='\\n\\n')\n",
    "#print(train_yellow, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LXf6fgfVjvH0"
   },
   "source": [
    "### Label Loading\n",
    "\n",
    "Loading the labels from *train.csv*. Here we seek to obtain all the labels for the train and test data. These labels could be multi-label, as in indicating more then one protein may be present in any data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtNBfjMfjxV2"
   },
   "outputs": [],
   "source": [
    "train_labels = {} # dictionary with key = photo ID and entry = list of labels\n",
    "\n",
    "# Get all the labels\n",
    "with open('dataset/train.csv') as label_file:\n",
    "    csvreader = csv.reader(label_file, delimiter=',', quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if \"Id\" not in row[0]:\n",
    "            # only take the ones that were taken in the Image Loading part\n",
    "            if any(row[0] in x for x in train_green):\n",
    "                train_labels[row[0]] = row[1:][0].split(' ') # labels are separated by spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8TTKKnnkewLO"
   },
   "source": [
    "### Image Preprocessing\n",
    "We now move onto some image preprocessing after reading the data image .csv files into lists. We will now define a function to produce the input labels and output labels in a unique vector style called one hot encoding. We will also load all the training data into a pandas.dataframe with their labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nhr-zmArr_Ag"
   },
   "source": [
    "#### One Hot Encoding\n",
    "One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.The categorical value represents the numerical value of the entry in the dataset. For example, if a training example is labelled as having NUCLEOLI_FIBRILLAR_CENTER (=3) and ENDOPLASMIC_RETICULUM  (=6), the one_hot_encoded label would be: [0 0 0 1 0 0 1 0....(28 places in total)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kV7Weri7sRWK"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(train_tags):\n",
    "    ''' Creates a one hot encoded dictionary of all labels for each pic'''\n",
    "    encoded = dict()\n",
    "  \n",
    "    for key in train_tags.keys():\n",
    "        # create empty vector\n",
    "        encoding = np.zeros(LEN_LABELS, dtype='uint8')\n",
    "        # mark 1 for each tag in the vector\n",
    "        for tag in train_tags[key]:\n",
    "            encoding[int(tag)] = 1\n",
    "            \n",
    "        encoded[key] = tuple(encoding)\n",
    "  \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load into Pandas\n",
    "We now load every data example, and its corresponding one-hot encoded label into a pandas.dataframe for easier reference later when recalling the label as an answer. We model the training set in this way when providing answers for training the model and testing the validation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Photo ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>003feb6e-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>09599842-bbc7-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1263dcf2-bba1-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17c12330-bbc9-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1a7de584-bbad-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1ec3408c-bbb5-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1edd9a1e-bbb4-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233a0498-bbc9-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2664d9c8-bba5-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27e40a9e-bba0-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      0   1   2   3   4   5   6   7   8   9   \\\n",
       "Photo ID                                                                       \n",
       "003feb6e-bbca-11e8-b2bc-ac1f6b6435d0   1   0   0   0   0   0   0   0   0   0   \n",
       "09599842-bbc7-11e8-b2bc-ac1f6b6435d0   0   0   1   0   0   0   0   0   0   0   \n",
       "1263dcf2-bba1-11e8-b2b9-ac1f6b6435d0   1   1   0   0   0   0   0   0   0   0   \n",
       "17c12330-bbc9-11e8-b2bc-ac1f6b6435d0   0   0   1   0   0   0   0   0   0   0   \n",
       "1a7de584-bbad-11e8-b2ba-ac1f6b6435d0   0   0   0   0   0   0   0   0   0   0   \n",
       "1ec3408c-bbb5-11e8-b2ba-ac1f6b6435d0   1   0   1   0   0   0   0   0   0   0   \n",
       "1edd9a1e-bbb4-11e8-b2ba-ac1f6b6435d0   0   0   0   0   0   0   0   1   0   0   \n",
       "233a0498-bbc9-11e8-b2bc-ac1f6b6435d0   0   0   0   0   0   0   0   0   0   0   \n",
       "2664d9c8-bba5-11e8-b2ba-ac1f6b6435d0   1   0   1   0   0   0   0   0   0   0   \n",
       "27e40a9e-bba0-11e8-b2b9-ac1f6b6435d0   1   0   0   0   0   0   0   0   0   0   \n",
       "\n",
       "                                      ...  18  19  20  21  22  23  24  25  26  \\\n",
       "Photo ID                              ...                                       \n",
       "003feb6e-bbca-11e8-b2bc-ac1f6b6435d0  ...   0   0   0   0   0   0   0   0   0   \n",
       "09599842-bbc7-11e8-b2bc-ac1f6b6435d0  ...   0   0   0   0   0   0   0   1   0   \n",
       "1263dcf2-bba1-11e8-b2b9-ac1f6b6435d0  ...   0   0   0   0   0   0   0   0   0   \n",
       "17c12330-bbc9-11e8-b2bc-ac1f6b6435d0  ...   0   0   0   0   0   1   0   0   0   \n",
       "1a7de584-bbad-11e8-b2ba-ac1f6b6435d0  ...   0   0   0   0   0   0   0   1   0   \n",
       "1ec3408c-bbb5-11e8-b2ba-ac1f6b6435d0  ...   0   0   0   0   0   0   0   0   0   \n",
       "1edd9a1e-bbb4-11e8-b2ba-ac1f6b6435d0  ...   0   0   0   0   0   0   0   0   0   \n",
       "233a0498-bbc9-11e8-b2bc-ac1f6b6435d0  ...   0   0   0   0   0   0   0   0   0   \n",
       "2664d9c8-bba5-11e8-b2ba-ac1f6b6435d0  ...   0   0   0   0   0   0   0   1   0   \n",
       "27e40a9e-bba0-11e8-b2b9-ac1f6b6435d0  ...   0   0   0   0   0   0   0   1   0   \n",
       "\n",
       "                                      27  \n",
       "Photo ID                                  \n",
       "003feb6e-bbca-11e8-b2bc-ac1f6b6435d0   0  \n",
       "09599842-bbc7-11e8-b2bc-ac1f6b6435d0   0  \n",
       "1263dcf2-bba1-11e8-b2b9-ac1f6b6435d0   0  \n",
       "17c12330-bbc9-11e8-b2bc-ac1f6b6435d0   0  \n",
       "1a7de584-bbad-11e8-b2ba-ac1f6b6435d0   0  \n",
       "1ec3408c-bbb5-11e8-b2ba-ac1f6b6435d0   0  \n",
       "1edd9a1e-bbb4-11e8-b2ba-ac1f6b6435d0   0  \n",
       "233a0498-bbc9-11e8-b2bc-ac1f6b6435d0   0  \n",
       "2664d9c8-bba5-11e8-b2ba-ac1f6b6435d0   0  \n",
       "27e40a9e-bba0-11e8-b2b9-ac1f6b6435d0   0  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_labels = one_hot_encode(train_labels)\n",
    "training_set = pd.DataFrame.from_dict(ohe_labels, orient='index')\n",
    "training_set.index.name = \"Photo ID\"\n",
    "training_set.reset_index()\n",
    "training_set.head(n=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Visualization (using matplotlib)\n",
    "\n",
    "As we can see the training set is highly imbalanced.\n",
    "This may be indicative of the rarity of certain protein types.\n",
    "\n",
    "Data imbalance is a well-known problem in Machine Learning. Where some classes in the dataset are more frequent than others, and the neural net just learns to predict the frequent classes.\n",
    "\n",
    "This is a big issue, to mitigate in this case, we can easily balance the data using sampling techniques.\n",
    "By removing some frequent examples(downsampling)\n",
    "By creating more in-frequent examples(upsampling) using image augmentation or any other method.\n",
    "\n",
    "Perhaps a larger data set will show more balanced results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x204e2b50828>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Feature Distribution\")\n",
    "plt.xlabel(\"# of occurences\")\n",
    "plt.ylabel(\"protein organelle\")\n",
    "# If this gives a boring graph, increase the subset size to look at more features.\n",
    "training_set.astype(bool).sum(axis=0).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "\n",
    "(see https://keras.io/preprocessing/image/)\n",
    "\n",
    "This will provide real-time data augmentation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation\n",
    "datagen = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "              height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "              horizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and Process (into Keras-ready train and test sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Keras and OpenCV docs\n",
    "\n",
    "def read_and_process(images_paths, ohe_dataframe):\n",
    "    '''\n",
    "    takes a panda dataframe with rows as picture IDs and columns as labels\n",
    "    returns lists for keras (to be used as x and y inputs to the CNN)\n",
    "    '''\n",
    "\n",
    "    images = [] # will be used by keras as x\n",
    "    labels = [] # will be used by keras as y\n",
    "\n",
    "    for img in images_paths:\n",
    "        # build image list using img_to_array() and OpenCV\n",
    "        images.append(img_to_array(cv2.imread(img, cv2.IMREAD_GRAYSCALE))) # load images using cv2.imread()\n",
    "\n",
    "        # build labels/target/tag list\n",
    "        # Check labels at each step and build labels list accordingly\n",
    "        # --> order is important, the lists are meant to be zip()ed\n",
    "        for idx, row in ohe_dataframe.iterrows():\n",
    "            if idx in img:\n",
    "                labels.append(tuple(row))\n",
    "    \n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and Train splitting\n",
    "\n",
    "Split the data from the train set into 80% training and 20% validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 512, 512, 1) (80, 28) (20, 512, 512, 1) (20, 28)\n"
     ]
    }
   ],
   "source": [
    "# WIP\n",
    "# USE sklearn.model_selection import train_test_split FOR THIS PART\n",
    "X, Y = read_and_process(train_green, training_set)\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "print(trainX.shape, trainY.shape, testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x, train_y = read_and_process(train_green, training_set)\n",
    "# test_x,  test_y  = read_and_process(validation_green, training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Setup\n",
    "\n",
    "Now that the preprocessing is out of the way, we can build our CNN structure.\n",
    "\n",
    "This will be a smaller implementation of *VGGNet*, as described here: https://arxiv.org/pdf/1409.1556/\n",
    "\n",
    "#### Input Layer Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "height, width, depth = IMG_SIZE\n",
    "input_shape = (height, width, depth) # derived from image size\n",
    "chanDim = -1\n",
    "classes = LEN_LABELS # number of classification classes\n",
    "\n",
    "# if we are using \"channels first\", update the input shape\n",
    "# and channels dimension\n",
    "if backend.image_data_format() == \"channels_first\":\n",
    "    input_shape = (depth, height, width)\n",
    "    chanDim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv + ReLU + Pool Blocks\n",
    "\n",
    "Increasing filter sizes help increase depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First CONV => RELU => POOL block\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=input_shape)) # 32 filters and a 3x3 kernel\n",
    "model.add(Activation(\"relu\")) # standard activation layer\n",
    "model.add(BatchNormalization(axis=chanDim)) # normalize all inputs to the [0, 1] range\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Dropout(0.25)) # dropout will reduce overfitting by randomly dropping 25% of node connections\n",
    "\n",
    "# (CONV => RELU) * 2 => POOL (first double convolution before pooling)\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\")) # 64 filters and 3x3 kernel\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # smaller pool size this time to reduce spatial size\n",
    "model.add(Dropout(0.25))\n",
    " \n",
    "# (CONV => RELU) * 2 => POOL (second double convolution before pooling)\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\")) # 128 filters and 3x3 kernel\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully Connected + ReLU block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first (and only) set of FC => RELU layers\n",
    "model.add(Flatten()) # Collapses the spatial dimensions of the input into the channel dimensions\n",
    "model.add(Dense(1024)) # dense layer of neurons with 1024 being the dimensionality of the output space\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5)) # Dropout of 50%\n",
    "\n",
    "# Since this is multi-label classification we use the sigmoid activation function\n",
    "model.add(Dense(classes)) # output dimensionality equal to the number of output classes\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "##I think we want \"softmax\" not sigmoid, any picture may be multiple 1's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = SGD(lr=0.01, momentum=0.9) # Stochastic Gradient Descent with 0.01 learning rate\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS) # Adam will provide results faster than SGD with similar quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FBeta Score Function\n",
    "\n",
    "We must define our performance metric before compiling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fbeta score for multi-class/label classification\n",
    "def fbeta(y_true, y_pred, beta=2):\n",
    "    # clip predictions\n",
    "    y_pred = backend.clip(y_pred, 0, 1)\n",
    "    # calculate elements\n",
    "    tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
    "    fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "    # calculate precision\n",
    "    p = tp / (tp + fp + backend.epsilon())\n",
    "    # calculate recall\n",
    "    r = tp / (tp + fn + backend.epsilon())\n",
    "    # calculate fbeta, averaged across each class\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
    "    return fbeta_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use binary cross-entropy as this is multilabel multicategory classification\n",
    "# For metrics we look at accuracy but also a more advanced metric for multilabel classification called the F-beta score\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', fbeta])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/15\n",
      "16/80 [=====>........................] - ETA: 3:39 - loss: 1.0595 - accuracy: 0.4888 - fbeta: 0.1301"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "\n",
    "# With data augmentation\n",
    "#hist = model.fit_generator(datagen.flow(trainX, trainY, batch_size=BS),\n",
    "#            validation_data=(testX, testY),\n",
    "#            steps_per_epoch=(len(trainX) // BS),\n",
    "#            epochs=EPOCHS, verbose=1)\n",
    "\n",
    "# Without data augmentation\n",
    "hist = model.fit(trainX, trainY,\n",
    "                batch_size=BS,\n",
    "                validation_data=(testX, testY),\n",
    "                epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagnostics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP\n",
    "def summarize_diagnostics(history):\n",
    "    # plot loss\n",
    "    plt.subplot(211)\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot(history.history['loss'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    # plot accuracy\n",
    "    plt.subplot(212)\n",
    "    plt.title('Fbeta')\n",
    "    plt.plot(history.history['fbeta'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_fbeta'], color='orange', label='test')\n",
    "    # save plot to file\n",
    "    filename = sys.argv[0].split('/')[-1]\n",
    "    plt.savefig(filename + '_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_diagnostics(hist)\n",
    "#precision = true positives / (true positives + false positives)\n",
    "#recall = true positives / (true positives + false negatives)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP -- Optional\n",
    "# Serialize the model using pickle (if possible) to avoid having to re-train it"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "protein_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
